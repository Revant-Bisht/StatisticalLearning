{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccc0f178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from utils.matrices import MyMatrixSolver\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b6f0212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_array = np.array([[1, 0, -2, 6], [-3, 6, 6, -6], [2, -3, -4, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c306546e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-28 15:22:22,071 - utils.matrices - WARNING - Matrix is not square. Returning NaN.\n",
      "2025-12-28 15:22:22,072 - utils.matrices - WARNING - Matrix is not square. Cannot compute inverse.\n",
      "2025-12-28 15:22:22,073 - utils.matrices - WARNING - Matrix is not square. Cannot compute trace.\n",
      "2025-12-28 15:22:22,073 - utils.matrices - WARNING - Matrix is not square. Cannot compute eigenvalues.\n",
      "2025-12-28 15:22:22,074 - utils.matrices - WARNING - Matrix is not square. Cannot compute eigenvalues.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1, -3,  2],\n",
       "       [ 0,  6, -3]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_solver = MyMatrixSolver(input_array)\n",
    "matrix_solver.basis\n",
    "\n",
    "# matrix_solver.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c5f32937",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_truncated = matrix_solver._truncate_matrix(input_array, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "14a5e9d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bcd55ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(-2)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_solver.determinant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f4b96c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_array_2 = np.array([[ 0,  1,  0, -2,  1],\n",
    "       [ 1,  0,  3,  1,  1],\n",
    "       [ 1, -1,  1,  1,  1],\n",
    "       [ 2,  2,  1,  0,  1],\n",
    "       [ 3,  1,  1,  1,  2]])\n",
    "matrix_solver_2  = MyMatrixSolver(input_array_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "48c69dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  0, -2,  1],\n",
       "       [ 1,  0,  3,  1,  1],\n",
       "       [ 1, -1,  1,  1,  1],\n",
       "       [ 2,  2,  1,  0,  1],\n",
       "       [ 3,  1,  1,  1,  2]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_array_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "95482ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2. ,  1. ],\n",
       "       [ 1.5, -0.5]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_solver.inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d72b37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after cleaning: (7906, 7)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/Users/kikirevi/RevantProjects/StatisticalLearning/utils/data/Car_details_v3.csv\")\n",
    "\n",
    "# 2. Clean numerical columns (remove units like 'kmpl', 'CC', 'bhp')\n",
    "def extract_numeric(series):\n",
    "    \"\"\"Extract numeric values from strings with units\"\"\"\n",
    "    return pd.to_numeric(series.astype(str).str.extract(r'([\\d.]+)')[0], errors='coerce')\n",
    "\n",
    "df['mileage_clean'] = extract_numeric(df['mileage'])\n",
    "df['engine_clean'] = extract_numeric(df['engine'])\n",
    "df['max_power_clean'] = extract_numeric(df['max_power'])\n",
    "\n",
    "# 3. Select features and target, drop rows with missing values\n",
    "features = ['year', 'km_driven', 'mileage_clean', 'engine_clean', 'max_power_clean', 'seats']\n",
    "target = 'selling_price'\n",
    "\n",
    "# Keep only rows without NaN in our selected columns\n",
    "df_clean = df[features + [target]].dropna()\n",
    "\n",
    "print(f\"Data shape after cleaning: {df_clean.shape}\")\n",
    "\n",
    "# 4. Create feature matrix X and target vector Y\n",
    "# Add a column of 1s for the intercept term (bias)\n",
    "X = df_clean[features].values\n",
    "X = np.column_stack([np.ones(X.shape[0]), X])  # Add intercept column\n",
    "Y = df_clean[target].values.reshape(-1, 1)      # Reshape to (N, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37af60de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Beta coefficients shape: (7, 1)\n",
      "Beta values:\n",
      "[-8.34562759e+07  4.11101700e+04 -1.41000000e+00  1.05143600e+04\n",
      "  1.14650000e+02  1.56545400e+04 -7.32929600e+04]\n"
     ]
    }
   ],
   "source": [
    "from utils.linear_regression import calculate_beta, calculate_residual_sum_of_squares\n",
    "\n",
    "Beta = calculate_beta(X, Y)\n",
    "print(f\"\\nBeta coefficients shape: {Beta.shape}\")\n",
    "print(f\"Beta values:\\n{Beta.flatten()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ad299fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  intercept: -83456275.8843\n",
      "  year: 41110.1734\n",
      "  km_driven: -1.4070\n",
      "  mileage_clean: 10514.3603\n",
      "  engine_clean: 114.6536\n",
      "  max_power_clean: 15654.5386\n",
      "  seats: -73292.9561\n",
      "\n",
      "Residual Sum of Squares (RSS): 1882969454267974.00\n",
      "R-squared: 0.6401\n"
     ]
    }
   ],
   "source": [
    "# Interpret coefficients\n",
    "feature_names = ['intercept'] + features\n",
    "for name, coef in zip(feature_names, Beta.flatten()):\n",
    "    print(f\"  {name}: {coef:.4f}\")\n",
    "\n",
    "# 6. Calculate Residual Sum of Squares\n",
    "RSS = calculate_residual_sum_of_squares(X, Y, Beta)\n",
    "print(f\"\\nResidual Sum of Squares (RSS): {RSS[0,0]:.2f}\")\n",
    "\n",
    "# 7. Calculate predictions and R-squared for validation\n",
    "Y_pred = X @ Beta\n",
    "SS_total = np.sum((Y - Y.mean())**2)\n",
    "SS_residual = RSS[0, 0]\n",
    "R_squared = 1 - (SS_residual / SS_total)\n",
    "print(f\"R-squared: {R_squared:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b35af24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COMPARISON: Your Implementation vs sklearn\n",
      "============================================================\n",
      "\n",
      "Feature                       Your Beta            sklearn         Diff\n",
      "--------------------------------------------------------------------\n",
      "intercept                -83456275.8843     -83456275.8833    -1.08e-03\n",
      "year                         41110.1734         41110.1734     5.36e-07\n",
      "km_driven                       -1.4070            -1.4070     1.12e-11\n",
      "mileage_clean                10514.3603         10514.3603    -2.91e-07\n",
      "engine_clean                   114.6536           114.6536     1.61e-09\n",
      "max_power_clean              15654.5386         15654.5386    -4.37e-08\n",
      "seats                       -73292.9561        -73292.9561    -8.88e-07\n",
      "\n",
      "Metric                       Your Value            sklearn\n",
      "--------------------------------------------------------\n",
      "R-squared                      0.640136           0.640136\n",
      "\n",
      "‚úÖ SUCCESS: Your coefficients match sklearn perfectly!\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# Compare with sklearn OLS\n",
    "# ========================================================\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Sklearn expects X without the intercept column (it adds it internally)\n",
    "X_sklearn = df_clean[features].values  # No intercept column\n",
    "y_sklearn = df_clean[target].values    # 1D array\n",
    "\n",
    "# Fit sklearn model\n",
    "sklearn_model = LinearRegression()\n",
    "sklearn_model.fit(X_sklearn, y_sklearn)\n",
    "\n",
    "# Get sklearn coefficients\n",
    "sklearn_intercept = sklearn_model.intercept_\n",
    "sklearn_coefs = sklearn_model.coef_\n",
    "\n",
    "# Compare coefficients\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPARISON: Your Implementation vs sklearn\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{'Feature':<20} {'Your Beta':>18} {'sklearn':>18} {'Diff':>12}\")\n",
    "print(\"-\" * 68)\n",
    "\n",
    "# Intercept\n",
    "your_intercept = Beta.flatten()[0]\n",
    "diff = your_intercept - sklearn_intercept\n",
    "print(f\"{'intercept':<20} {your_intercept:>18.4f} {sklearn_intercept:>18.4f} {diff:>12.2e}\")\n",
    "\n",
    "# Other coefficients\n",
    "for i, name in enumerate(features):\n",
    "    your_coef = Beta.flatten()[i + 1]\n",
    "    sk_coef = sklearn_coefs[i]\n",
    "    diff = your_coef - sk_coef\n",
    "    print(f\"{name:<20} {your_coef:>18.4f} {sk_coef:>18.4f} {diff:>12.2e}\")\n",
    "\n",
    "# Compare R-squared\n",
    "sklearn_r2 = sklearn_model.score(X_sklearn, y_sklearn)\n",
    "print(f\"\\n{'Metric':<20} {'Your Value':>18} {'sklearn':>18}\")\n",
    "print(\"-\" * 56)\n",
    "print(f\"{'R-squared':<20} {R_squared:>18.6f} {sklearn_r2:>18.6f}\")\n",
    "\n",
    "# Check if coefficients match (within floating point tolerance)\n",
    "your_all_coefs = Beta.flatten()\n",
    "sklearn_all_coefs = np.concatenate([[sklearn_intercept], sklearn_coefs])\n",
    "if np.allclose(your_all_coefs, sklearn_all_coefs, rtol=1e-10):\n",
    "    print(\"\\n‚úÖ SUCCESS: Your coefficients match sklearn perfectly!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Coefficients have small differences (likely floating point precision)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38c1b731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-29 17:42:16,878 - utils.matrices - WARNING - Matrix is not square. Returning NaN.\n",
      "2025-12-29 17:42:16,878 - utils.matrices - WARNING - Matrix is not square. Cannot compute inverse.\n",
      "2025-12-29 17:42:16,881 - utils.matrices - WARNING - Matrix is not square. Cannot compute trace.\n",
      "2025-12-29 17:42:16,883 - utils.matrices - WARNING - Matrix is not square. Cannot compute eigenvalues.\n",
      "2025-12-29 17:42:16,884 - utils.matrices - WARNING - Matrix is not square. Cannot compute eigenvalues.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "  1. SAMPLE DATA GENERATION\n",
      "======================================================================\n",
      "Data shape: (100, 5) (samples x features)\n",
      "Feature variances: [7.423 4.694 4.703 1.676 0.283]\n",
      "Data preview (first 5 rows):\n",
      "   Feature_1  Feature_2  Feature_3  Feature_4  Feature_5\n",
      "0   1.490142   0.484429   0.715575  -0.305409  -0.797214\n",
      "1  -0.414793  -0.542157   1.121569   0.112640  -0.299688\n",
      "2   1.943066   1.383095   2.166102   1.680886   0.002622\n",
      "3   4.569090   3.254133   2.107604   1.542098   0.023490\n",
      "4  -0.702460  -0.642611  -2.755339  -1.394391  -0.225033\n",
      "\n",
      "======================================================================\n",
      "  2. PCA COMPUTATION\n",
      "======================================================================\n",
      "\n",
      "--- Custom Implementation ---\n",
      "Mean vector: [-0.31154  -0.238079  0.129793  0.150368 -0.028002]\n",
      "Explained variance (eigenvalues): [12.302895  5.683085  0.411867  0.253305  0.12733 ]\n",
      "\n",
      "--- Sklearn Implementation ---\n",
      "Mean vector: [-0.31154  -0.238079  0.129793  0.150368 -0.028002]\n",
      "Explained variance (eigenvalues): [12.302895  5.683085  0.411867  0.253305  0.12733 ]\n",
      "\n",
      "======================================================================\n",
      "  3. COMPARISON: CUSTOM vs SKLEARN\n",
      "======================================================================\n",
      "\n",
      "--- Mean Vectors ---\n",
      "Custom:  [-0.31153955 -0.23807935  0.12979251  0.15036835 -0.02800224]\n",
      "Sklearn: [-0.31153955 -0.23807935  0.12979251  0.15036835 -0.02800224]\n",
      "Max difference: 0.00e+00\n",
      "\n",
      "--- Explained Variance (Eigenvalues) ---\n",
      " PC    Custom   Sklearn  Difference\n",
      "PC1 12.302895 12.302895    0.000000\n",
      "PC2  5.683085  5.683085    0.000000\n",
      "PC3  0.411867  0.411867    0.000000\n",
      "PC4  0.253305  0.253305    0.000000\n",
      "PC5  0.127330  0.127330    0.000000\n",
      "\n",
      "Max difference: 1.24e-14\n",
      "\n",
      "--- Explained Variance Ratio ---\n",
      " PC  Custom (%)  Sklearn (%)  Difference\n",
      "PC1   65.515921    65.515921    0.000000\n",
      "PC2   30.263816    30.263816    0.000000\n",
      "PC3    2.193292     2.193292    0.000000\n",
      "PC4    1.348910     1.348910    0.000000\n",
      "PC5    0.678062     0.678062    0.000000\n",
      "\n",
      "Max difference: 4.44e-16\n",
      "\n",
      "--- Principal Components (Eigenvectors) ---\n",
      "Note: Eigenvectors may differ by sign (both are valid)\n",
      "\n",
      "Custom Principal Components (shape (5, 5)):\n",
      "               PC1       PC2       PC3       PC4       PC5\n",
      "Feature1 -0.765800  0.164700  0.005200 -0.178800  0.595300\n",
      "Feature2 -0.605000  0.136500  0.061000  0.222000 -0.749900\n",
      "Feature3 -0.204900 -0.849100 -0.425800  0.231700  0.044700\n",
      "Feature4 -0.071600 -0.482800  0.776200 -0.390500 -0.082600\n",
      "Feature5  0.020100  0.008700  0.461000  0.844100  0.272800\n",
      "\n",
      "Sklearn Principal Components:\n",
      "               PC1       PC2       PC3       PC4       PC5\n",
      "Feature1  0.765800 -0.164700  0.005200 -0.178800 -0.595300\n",
      "Feature2  0.605000 -0.136500  0.061000  0.222000  0.749900\n",
      "Feature3  0.204900  0.849100 -0.425800  0.231700 -0.044700\n",
      "Feature4  0.071600  0.482800  0.776200 -0.390500  0.082600\n",
      "Feature5 -0.020100 -0.008700  0.461000  0.844100 -0.272800\n",
      "\n",
      "Max absolute difference: 1.61e-15\n",
      "\n",
      "--- Transformed Data (First 5 Samples) ---\n",
      "Note: Signs may be flipped per component\n",
      "\n",
      "Custom transformed data:\n",
      "        PC1       PC2       PC3       PC4      PC5\n",
      "0 -1.919700  0.111300 -0.904300 -0.497300 0.384700\n",
      "1  0.057100 -0.884800 -0.595900 -0.033800 0.139800\n",
      "2 -3.233600 -1.875200  0.445700 -0.143100 0.099400\n",
      "3 -6.354300 -1.070500  0.500400 -0.138900 0.274200\n",
      "4  1.241800  3.074400 -0.088100 -0.251600 0.015600\n",
      "\n",
      "Sklearn transformed data:\n",
      "        PC1       PC2       PC3       PC4       PC5\n",
      "0  1.919700 -0.111300 -0.904300 -0.497300 -0.384700\n",
      "1 -0.057100  0.884800 -0.595900 -0.033800 -0.139800\n",
      "2  3.233600  1.875200  0.445700 -0.143100 -0.099400\n",
      "3  6.354300  1.070500  0.500400 -0.138900 -0.274200\n",
      "4 -1.241800 -3.074400 -0.088100 -0.251600 -0.015600\n",
      "\n",
      "Max absolute difference: 5.33e-15\n",
      "\n",
      "======================================================================\n",
      "  4. EXPLAINED VARIANCE SUMMARY (Custom Implementation)\n",
      "======================================================================\n",
      " Principal Component  Eigenvalue  Variance Ratio  Variance %  Cumulative %\n",
      "                   1   12.302895        0.655159   65.515921     65.515921\n",
      "                   2    5.683085        0.302638   30.263816     95.779737\n",
      "                   3    0.411867        0.021933    2.193292     97.973028\n",
      "                   4    0.253305        0.013489    1.348910     99.321938\n",
      "                   5    0.127330        0.006781    0.678062    100.000000\n",
      "\n",
      "======================================================================\n",
      "  5. SCREE PLOT\n",
      "======================================================================\n",
      "Generating scree plot...\n",
      "Scree plot saved to: pca_scree_comparison.png\n",
      "\n",
      "======================================================================\n",
      "  6. OVERALL SUMMARY\n",
      "======================================================================\n",
      "‚úÖ Mean vectors match\n",
      "‚úÖ Explained variances (eigenvalues) match\n",
      "‚úÖ Explained variance ratios match\n",
      "‚úÖ Principal components match (up to sign)\n",
      "‚úÖ Transformed data matches (up to sign)\n",
      "\n",
      "üéâ Custom PCA implementation is equivalent to sklearn!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PCA Comparison Demo: Custom Implementation vs Sklearn\n",
    "======================================================\n",
    "This script compares the custom PCA implementation with sklearn's PCA\n",
    "and displays visual outputs including scree plots and summary tables.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use non-interactive backend for saving plots\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA as SklearnPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Import custom PCA functions\n",
    "from utils.PCA import (\n",
    "    calculate_pca_parameters,\n",
    "    apply_pca_transform,\n",
    "    explained_variance_summary,\n",
    "    scree_plot\n",
    ")\n",
    "\n",
    "# Set display options for pandas\n",
    "pd.set_option('display.float_format', '{:.6f}'.format)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "def print_header(title: str):\n",
    "    \"\"\"Print a formatted section header.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"  {title}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "def print_subheader(title: str):\n",
    "    \"\"\"Print a formatted subsection header.\"\"\"\n",
    "    print(f\"\\n--- {title} ---\")\n",
    "\n",
    "def compare_pca_implementations():\n",
    "    \"\"\"Run PCA comparison between custom implementation and sklearn.\"\"\"\n",
    "    \n",
    "    # =========================================================================\n",
    "    # SECTION 1: Generate Sample Data\n",
    "    # =========================================================================\n",
    "    print_header(\"1. SAMPLE DATA GENERATION\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    n_samples = 100\n",
    "    \n",
    "    # Create correlated features (ideal for PCA demonstration)\n",
    "    x1 = np.random.randn(n_samples) * 3  # High variance\n",
    "    x2 = x1 * 0.8 + np.random.randn(n_samples) * 0.5  # Correlated with x1\n",
    "    x3 = np.random.randn(n_samples) * 2  # Medium variance, independent\n",
    "    x4 = x3 * 0.5 + np.random.randn(n_samples) * 0.8  # Correlated with x3\n",
    "    x5 = np.random.randn(n_samples) * 0.5  # Low variance\n",
    "    \n",
    "    X = np.column_stack([x1, x2, x3, x4, x5])\n",
    "    \n",
    "    print(f\"Data shape: {X.shape} (samples x features)\")\n",
    "    print(f\"Feature variances: {np.var(X, axis=0, ddof=1).round(3)}\")\n",
    "    print(f\"Data preview (first 5 rows):\")\n",
    "    print(pd.DataFrame(X, columns=[f'Feature_{i+1}' for i in range(5)]).head())\n",
    "    \n",
    "    # =========================================================================\n",
    "    # SECTION 2: Run Both PCA Implementations\n",
    "    # =========================================================================\n",
    "    print_header(\"2. PCA COMPUTATION\")\n",
    "    \n",
    "    # Custom PCA\n",
    "    print_subheader(\"Custom Implementation\")\n",
    "    principal_components, explained_variance, mean_vector, _ = calculate_pca_parameters(X)\n",
    "    X_custom = apply_pca_transform(X, principal_components, mean_vector)\n",
    "    \n",
    "    print(f\"Mean vector: {mean_vector.round(6)}\")\n",
    "    print(f\"Explained variance (eigenvalues): {explained_variance.round(6)}\")\n",
    "    \n",
    "    # Sklearn PCA\n",
    "    print_subheader(\"Sklearn Implementation\")\n",
    "    sklearn_pca = SklearnPCA()\n",
    "    X_sklearn = sklearn_pca.fit_transform(X)\n",
    "    \n",
    "    print(f\"Mean vector: {sklearn_pca.mean_.round(6)}\")\n",
    "    print(f\"Explained variance (eigenvalues): {sklearn_pca.explained_variance_.round(6)}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # SECTION 3: Compare Results\n",
    "    # =========================================================================\n",
    "    print_header(\"3. COMPARISON: CUSTOM vs SKLEARN\")\n",
    "    \n",
    "    # Mean vectors\n",
    "    print_subheader(\"Mean Vectors\")\n",
    "    mean_diff = np.abs(mean_vector - sklearn_pca.mean_)\n",
    "    print(f\"Custom:  {mean_vector.round(8)}\")\n",
    "    print(f\"Sklearn: {sklearn_pca.mean_.round(8)}\")\n",
    "    print(f\"Max difference: {mean_diff.max():.2e}\")\n",
    "    \n",
    "    # Explained variance\n",
    "    print_subheader(\"Explained Variance (Eigenvalues)\")\n",
    "    var_diff = np.abs(explained_variance - sklearn_pca.explained_variance_)\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'PC': [f'PC{i+1}' for i in range(len(explained_variance))],\n",
    "        'Custom': explained_variance,\n",
    "        'Sklearn': sklearn_pca.explained_variance_,\n",
    "        'Difference': var_diff\n",
    "    })\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    print(f\"\\nMax difference: {var_diff.max():.2e}\")\n",
    "    \n",
    "    # Variance ratios\n",
    "    print_subheader(\"Explained Variance Ratio\")\n",
    "    custom_ratio = explained_variance / np.sum(explained_variance)\n",
    "    ratio_diff = np.abs(custom_ratio - sklearn_pca.explained_variance_ratio_)\n",
    "    ratio_df = pd.DataFrame({\n",
    "        'PC': [f'PC{i+1}' for i in range(len(custom_ratio))],\n",
    "        'Custom (%)': custom_ratio * 100,\n",
    "        'Sklearn (%)': sklearn_pca.explained_variance_ratio_ * 100,\n",
    "        'Difference': ratio_diff\n",
    "    })\n",
    "    print(ratio_df.to_string(index=False))\n",
    "    print(f\"\\nMax difference: {ratio_diff.max():.2e}\")\n",
    "    \n",
    "    # Principal components (compare absolute values due to sign ambiguity)\n",
    "    print_subheader(\"Principal Components (Eigenvectors)\")\n",
    "    print(\"Note: Eigenvectors may differ by sign (both are valid)\")\n",
    "    sklearn_components = sklearn_pca.components_.T  # Transpose to match our format\n",
    "    pc_diff = np.abs(np.abs(principal_components) - np.abs(sklearn_components))\n",
    "    print(f\"\\nCustom Principal Components (shape {principal_components.shape}):\")\n",
    "    print(pd.DataFrame(principal_components.round(4), \n",
    "                       columns=[f'PC{i+1}' for i in range(principal_components.shape[1])],\n",
    "                       index=[f'Feature{i+1}' for i in range(principal_components.shape[0])]))\n",
    "    print(f\"\\nSklearn Principal Components:\")\n",
    "    print(pd.DataFrame(sklearn_components.round(4),\n",
    "                       columns=[f'PC{i+1}' for i in range(sklearn_components.shape[1])],\n",
    "                       index=[f'Feature{i+1}' for i in range(sklearn_components.shape[0])]))\n",
    "    print(f\"\\nMax absolute difference: {pc_diff.max():.2e}\")\n",
    "    \n",
    "    # Transformed data (first 5 samples)\n",
    "    print_subheader(\"Transformed Data (First 5 Samples)\")\n",
    "    print(\"Note: Signs may be flipped per component\")\n",
    "    print(\"\\nCustom transformed data:\")\n",
    "    print(pd.DataFrame(X_custom[:5].round(4), \n",
    "                       columns=[f'PC{i+1}' for i in range(X_custom.shape[1])]))\n",
    "    print(\"\\nSklearn transformed data:\")\n",
    "    print(pd.DataFrame(X_sklearn[:5].round(4),\n",
    "                       columns=[f'PC{i+1}' for i in range(X_sklearn.shape[1])]))\n",
    "    \n",
    "    transform_diff = np.abs(np.abs(X_custom) - np.abs(X_sklearn))\n",
    "    print(f\"\\nMax absolute difference: {transform_diff.max():.2e}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # SECTION 4: Explained Variance Summary Table\n",
    "    # =========================================================================\n",
    "    print_header(\"4. EXPLAINED VARIANCE SUMMARY (Custom Implementation)\")\n",
    "    \n",
    "    summary_df = explained_variance_summary(explained_variance)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    # =========================================================================\n",
    "    # SECTION 5: Scree Plot\n",
    "    # =========================================================================\n",
    "    print_header(\"5. SCREE PLOT\")\n",
    "    print(\"Generating scree plot...\")\n",
    "    \n",
    "    # Create side-by-side scree plots for comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Custom implementation scree plot\n",
    "    x = np.arange(1, len(explained_variance) + 1)\n",
    "    variance_ratio = explained_variance / np.sum(explained_variance) * 100\n",
    "    cumulative = np.cumsum(variance_ratio)\n",
    "    \n",
    "    ax1 = axes[0]\n",
    "    ax1.bar(x, variance_ratio, alpha=0.7, color='steelblue', label='Individual')\n",
    "    ax1.plot(x, cumulative, 'ro-', linewidth=2, markersize=8, label='Cumulative')\n",
    "    ax1.axhline(y=80, color='gray', linestyle='--', alpha=0.7, label='80% threshold')\n",
    "    ax1.set_xlabel('Principal Component', fontsize=12)\n",
    "    ax1.set_ylabel('Explained Variance (%)', fontsize=12)\n",
    "    ax1.set_title('Custom PCA Implementation', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.legend(loc='center right')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Sklearn scree plot\n",
    "    sklearn_variance_ratio = sklearn_pca.explained_variance_ratio_ * 100\n",
    "    sklearn_cumulative = np.cumsum(sklearn_variance_ratio)\n",
    "    \n",
    "    ax2 = axes[1]\n",
    "    ax2.bar(x, sklearn_variance_ratio, alpha=0.7, color='darkorange', label='Individual')\n",
    "    ax2.plot(x, sklearn_cumulative, 'ro-', linewidth=2, markersize=8, label='Cumulative')\n",
    "    ax2.axhline(y=80, color='gray', linestyle='--', alpha=0.7, label='80% threshold')\n",
    "    ax2.set_xlabel('Principal Component', fontsize=12)\n",
    "    ax2.set_ylabel('Explained Variance (%)', fontsize=12)\n",
    "    ax2.set_title('Sklearn PCA Implementation', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.legend(loc='center right')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('PCA Scree Plot Comparison', fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.savefig('pca_scree_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    print(\"Scree plot saved to: pca_scree_comparison.png\")\n",
    "    plt.close()  # Close instead of show for non-interactive mode\n",
    "    \n",
    "    # =========================================================================\n",
    "    # SECTION 6: Overall Summary\n",
    "    # =========================================================================\n",
    "    print_header(\"6. OVERALL SUMMARY\")\n",
    "    \n",
    "    print(\"‚úÖ Mean vectors match\")\n",
    "    print(\"‚úÖ Explained variances (eigenvalues) match\")\n",
    "    print(\"‚úÖ Explained variance ratios match\")\n",
    "    print(\"‚úÖ Principal components match (up to sign)\")\n",
    "    print(\"‚úÖ Transformed data matches (up to sign)\")\n",
    "    print(\"\\nüéâ Custom PCA implementation is equivalent to sklearn!\")\n",
    "    \n",
    "    return X, principal_components, explained_variance, mean_vector\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    compare_pca_implementations()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (statlearning)",
   "language": "python",
   "name": "statlearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
